{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57fba083-ae77-4963-8cab-3432ce9cbd7a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92ab588d-ef5f-43d5-abf0-85eaa2d5f026",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Using cached wandb-0.19.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.12/site-packages (from wandb) (8.1.8)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.12/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.12/site-packages (from wandb) (5.28.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.12/site-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /opt/conda/lib/python3.12/site-packages (from wandb) (2.10.6)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from wandb) (2.32.3)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.24.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Using cached setproctitle-1.3.5-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from wandb) (75.8.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/conda/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb) (4.12.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Using cached wandb-0.19.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.8 MB)\n",
      "Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Downloading sentry_sdk-2.24.1-py2.py3-none-any.whl (336 kB)\n",
      "Using cached setproctitle-1.3.5-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Installing collected packages: setproctitle, sentry-sdk, docker-pycreds, wandb\n",
      "Successfully installed docker-pycreds-0.4.0 sentry-sdk-2.24.1 setproctitle-1.3.5 wandb-0.19.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2698210-718a-4ea6-b9fe-90bdadcbed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "228fa807-7a2c-42a1-a8ca-9de639ef141c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103313, 63) (44277, 63) (103313,) (44277,)\n",
      "(22138, 63) (22139, 63) (22138,) (22139,)\n",
      "X_train0  [ 4.74841505e-01  8.52257133e-01  1.46229922e-06  5.43130815e-01\n",
      "  7.87584066e-01 -7.35012516e-02  5.51918805e-01  6.86782658e-01\n",
      " -1.08694024e-01  4.63680536e-01  6.33455217e-01 -1.39162436e-01\n",
      "  3.75922471e-01  6.09890461e-01 -1.65406048e-01  5.25304854e-01\n",
      "  5.12442410e-01 -5.95139042e-02  5.63664794e-01  3.69527698e-01\n",
      " -1.12695187e-01  5.84739983e-01  2.74222910e-01 -1.47435576e-01\n",
      "  5.97175360e-01  1.98159873e-01 -1.66424483e-01  4.38691258e-01\n",
      "  5.25739908e-01 -5.98610975e-02  3.80971014e-01  3.72085869e-01\n",
      " -1.20647617e-01  3.46378207e-01  2.70861030e-01 -1.59385324e-01\n",
      "  3.15379590e-01  1.83500409e-01 -1.74398318e-01  3.74877930e-01\n",
      "  5.80185652e-01 -6.73106834e-02  3.61128092e-01  5.29071808e-01\n",
      " -1.56928211e-01  4.08112168e-01  6.01952732e-01 -1.73931554e-01\n",
      "  4.32187468e-01  6.50748730e-01 -1.55184045e-01  3.29831362e-01\n",
      "  6.52131617e-01 -7.87678286e-02  3.25864255e-01  6.04194522e-01\n",
      " -1.43662542e-01  3.64264727e-01  6.51097715e-01 -1.36605576e-01\n",
      "  3.86044264e-01  6.87089622e-01 -1.11509219e-01]\n",
      "Encoder: 22\n",
      "X_train0  [-0.14409181  0.97627613  1.35064229  0.24285501  0.99964528 -0.5833287\n",
      "  0.2953147   0.9316651  -0.47930831 -0.21967023  0.85458174 -0.42988094\n",
      " -0.69308043  0.74726016 -0.38630752  0.19540619  0.52824147 -0.17688858\n",
      "  0.43654401 -0.05427969 -0.14882095  0.50030596 -0.70218994 -0.14920302\n",
      "  0.49435292 -0.9513238  -0.14855248 -0.49333923  0.54173687 -0.14789362\n",
      " -0.88234453 -0.16789072 -0.1309146  -1.04753766 -0.89293535 -0.24910232\n",
      " -1.11704326 -1.16000137 -0.31373966 -0.85415464  0.71687904 -0.10420099\n",
      " -0.89206102  0.65014668 -0.35043247 -0.61472954  0.75129988 -0.47526191\n",
      " -0.42991866  0.78166519 -0.39982409 -0.91840378  0.89376961 -0.04297662\n",
      " -0.90769314  0.83231145 -0.29224907 -0.72728907  0.95339554 -0.24339432\n",
      " -0.59232106  0.98834825 -0.09789881]\n",
      "Using device: cuda\n",
      "Label Mapping (Letter → Number):\n",
      "{np.str_('A'): np.int64(0), np.str_('B'): np.int64(1), np.str_('C'): np.int64(2), np.str_('D'): np.int64(3), np.str_('E'): np.int64(4), np.str_('F'): np.int64(5), np.str_('G'): np.int64(6), np.str_('H'): np.int64(7), np.str_('I'): np.int64(8), np.str_('J'): np.int64(9), np.str_('K'): np.int64(10), np.str_('L'): np.int64(11), np.str_('M'): np.int64(12), np.str_('N'): np.int64(13), np.str_('O'): np.int64(14), np.str_('P'): np.int64(15), np.str_('Q'): np.int64(16), np.str_('R'): np.int64(17), np.str_('S'): np.int64(18), np.str_('SPACE'): np.int64(19), np.str_('T'): np.int64(20), np.str_('U'): np.int64(21), np.str_('V'): np.int64(22), np.str_('W'): np.int64(23), np.str_('X'): np.int64(24), np.str_('Y'): np.int64(25), np.str_('Z'): np.int64(26)}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "# Luka's code\n",
    "#Path\n",
    "dataset_path = '/exchange/dspro2/silent-speech/full-dataset/'\n",
    "\n",
    "#Load the dataset\n",
    "X = np.load(dataset_path + \"X_landmarks.npy\")\n",
    "y = np.load(dataset_path + \"y_labels.npy\")\n",
    "\n",
    "#Ensure all labels are in uppercase before encoding\n",
    "y = np.array([label.upper() for label in y])\n",
    "\n",
    "#Split into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)  # 70% Train\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # 15% Validation, 15% Test\n",
    "print(X_train.shape, X_temp.shape, y_train.shape, y_temp.shape)\n",
    "print(X_val.shape, X_test.shape, y_val.shape, y_test.shape)\n",
    "print(f'X_train0  {X_train[0]}')\n",
    "\n",
    "#Convert labels to numbers (A-Z → 0-26)\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(np.concatenate((y_train, y_val, y_test)))  # Fit on full dataset\n",
    "y_train = label_encoder.transform(y_train)\n",
    "y_val = label_encoder.transform(y_val)\n",
    "y_test = label_encoder.transform(y_test)\n",
    "print(f'Encoder: {y_train[0]}')\n",
    "\n",
    "# Normalize the landmarks (Standardization: mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))  # Flatten before fitting\n",
    "X_val = scaler.transform(X_val.reshape(X_val.shape[0], -1))\n",
    "X_test = scaler.transform(X_test.reshape(X_test.shape[0], -1))\n",
    "# No need to normalize y sets, since it is categorical data.\n",
    "print(f'X_train0  {X_train[0]}')\n",
    "\n",
    "#Convert to PyTorch tensors\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "# To check encoder\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "\n",
    "print(\"Label Mapping (Letter → Number):\")\n",
    "print(label_mapping)\n",
    "print(len(label_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a32d740-92fc-49d2-9bad-a6e454b504b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147590, 63)\n",
      "(103313, 63)\n",
      "(22138, 63)\n",
      "(22139, 63)\n",
      "147590\n",
      "A (147590,)\n",
      "(103313,)\n",
      "(22138,)\n",
      "(22139,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "dataset_path = '/exchange/dspro2/silent-speech/full-dataset/'\n",
    "\n",
    "X = np.load(dataset_path + \"X_landmarks.npy\")\n",
    "\n",
    "X_train = np.load(dataset_path + \"X_train.npy\")\n",
    "X_val = np.load(dataset_path + \"X_val.npy\")\n",
    "X_test = np.load(dataset_path + \"X_test.npy\")\n",
    "y_train = np.load(dataset_path + \"y_train.npy\")\n",
    "y_val = np.load(dataset_path + \"y_val.npy\")\n",
    "y_test = np.load(dataset_path + \"y_test.npy\")\n",
    "y = np.load(dataset_path + \"y_labels.npy\")\n",
    "y = np.array([label.upper() for label in y])\n",
    "print(X.shape)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "print(X_train.shape[0]+X_val.shape[0]+ X_test.shape[0])\n",
    "print(y[0], y.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Normalize the landmarks (Standardization: mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))  # Flatten before fitting\n",
    "X_val = scaler.transform(X_val.reshape(X_val.shape[0], -1))\n",
    "X_test = scaler.transform(X_test.reshape(X_test.shape[0], -1))\n",
    "# No need to normalize y sets, since it is categorical data.\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "673475af-d697-45e7-977c-d2bddfc4b0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103313, 63)\n",
      "torch.Size([103313, 63])\n",
      "[-0.14409181  0.97627613  1.35064229  0.24285501  0.99964528 -0.5833287\n",
      "  0.2953147   0.9316651  -0.47930831 -0.21967023  0.85458174 -0.42988094\n",
      " -0.69308043  0.74726016 -0.38630752  0.19540619  0.52824147 -0.17688858\n",
      "  0.43654401 -0.05427969 -0.14882095  0.50030596 -0.70218994 -0.14920302\n",
      "  0.49435292 -0.9513238  -0.14855248 -0.49333923  0.54173687 -0.14789362\n",
      " -0.88234453 -0.16789072 -0.1309146  -1.04753766 -0.89293535 -0.24910232\n",
      " -1.11704326 -1.16000137 -0.31373966 -0.85415464  0.71687904 -0.10420099\n",
      " -0.89206102  0.65014668 -0.35043247 -0.61472954  0.75129988 -0.47526191\n",
      " -0.42991866  0.78166519 -0.39982409 -0.91840378  0.89376961 -0.04297662\n",
      " -0.90769314  0.83231145 -0.29224907 -0.72728907  0.95339554 -0.24339432\n",
      " -0.59232106  0.98834825 -0.09789881]\n",
      "tensor([-0.1441,  0.9763,  1.3506,  0.2429,  0.9996, -0.5833,  0.2953,  0.9317,\n",
      "        -0.4793, -0.2197,  0.8546, -0.4299, -0.6931,  0.7473, -0.3863,  0.1954,\n",
      "         0.5282, -0.1769,  0.4365, -0.0543, -0.1488,  0.5003, -0.7022, -0.1492,\n",
      "         0.4944, -0.9513, -0.1486, -0.4933,  0.5417, -0.1479, -0.8823, -0.1679,\n",
      "        -0.1309, -1.0475, -0.8929, -0.2491, -1.1170, -1.1600, -0.3137, -0.8542,\n",
      "         0.7169, -0.1042, -0.8921,  0.6501, -0.3504, -0.6147,  0.7513, -0.4753,\n",
      "        -0.4299,  0.7817, -0.3998, -0.9184,  0.8938, -0.0430, -0.9077,  0.8323,\n",
      "        -0.2922, -0.7273,  0.9534, -0.2434, -0.5923,  0.9883, -0.0979],\n",
      "       device='cuda:0')\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train_tensor.shape)\n",
    "\n",
    "print(X_train[0])\n",
    "print(X_train_tensor[0])\n",
    "print(len(torch.unique(y_train_tensor)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592944eb-66a7-4906-8011-60d9d9867df9",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2776225b-7d87-4be9-88c5-2491d7e57c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AslCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(AslCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1)\n",
    "        self.fc1 = nn.Linear(32 * 21 * 3, 128)  # Fully connected layer\n",
    "        self.fc2 = nn.Linear(128, num_classes)  # Output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # No activation since CrossEntropyLoss applies Softmax\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a3e93b2-f05d-4e86-a54d-b30dcb9b439d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc637fc7e30>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reproducibility --> TODO CHECK IF WORKING\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51230968-dda1-44ce-8733-23ba0bddcbd7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Attempt with shuffled y to search bias in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ca1f0c-db68-44d6-a887-f6e53f4af38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "y_shuffled = y_train_tensor[torch.randperm(y_train_tensor.size(0))]  # Shuffle labels\n",
    "\n",
    "if not isinstance(y_shuffled, torch.Tensor):\n",
    "    y_shuffled = torch.tensor(y_shuffled, dtype=torch.long).to(device)\n",
    "    \n",
    "# Create a new dataset with shuffled labels\n",
    "shuffled_dataset = TensorDataset(X_train_tensor, y_shuffled)\n",
    "\n",
    "# Load into DataLoader\n",
    "shuffled_loader = DataLoader(shuffled_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da192ef5-d2dd-491f-a14a-f7e2ee3b5f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshse13\u001b[0m (\u001b[33mshse13-doe\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac10a28-cca7-41f7-8564-b5ce7c5e1e67",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fdfc750-da1f-4633-9d15-30f4edced6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# Create DataLoaders\n",
    "X_train_tensor = X_train_tensor.view(-1, 1, 21, 3)  # Adjust this based on expected shape (4D and not 2D)\n",
    "X_val_tensor = X_val_tensor.view(-1, 1, 21, 3)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46dbc22-1331-4c30-847f-5ccb9428fe2b",
   "metadata": {},
   "source": [
    "## Training, validation, test loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16db09ae-0e9d-4826-b09f-46d47c634e1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m precision_score, recall_score, confusion_matrix\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define Model, Loss, and Optimizer\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39munique(y_train_tensor)) \u001b[38;5;66;03m# 27\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m AslCNN(num_classes)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# Define Model, Loss, and Optimizer\n",
    "num_classes = len(torch.unique(y_train_tensor)) # 27\n",
    "model = AslCNN(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Save metrics for accuracy\n",
    "train_predictions = []\n",
    "train_labels = []\n",
    "train_loss = []\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader: \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch) # goes in the AslCNN\n",
    "        \n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        train_loss.append(total_loss/len(train_loader))\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b6e063e-7916-4559-b655-e845b5969029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0942\n",
      "Epoch 2, Loss: 0.0103\n",
      "Epoch 3, Loss: 0.0057\n",
      "Epoch 4, Loss: 0.0039\n",
      "Epoch 5, Loss: 0.0029\n",
      "Epoch 6, Loss: 0.0020\n",
      "Epoch 7, Loss: 0.0019\n",
      "Epoch 8, Loss: 0.0017\n",
      "Epoch 9, Loss: 0.0013\n",
      "Epoch 10, Loss: 0.0018\n",
      "Training complete!\n",
      "Epoch\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# Define Model, Loss, and Optimizer\n",
    "num_classes = len(torch.unique(y_train_tensor))  # 27\n",
    "model = AslCNN(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Save metrics for accuracy per epoch\n",
    "train_predictions = []  # To store predicted labels\n",
    "train_labels = []       # To store true labels\n",
    "train_loss = []         # To store loss values per batch\n",
    "epoch_losses = []       # To store average loss per epoch\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_losses = []  # To accumulate batch losses per epoch\n",
    "    \n",
    "    for X_batch, y_batch in train_loader: \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)  # Get raw outputs from the model (logits)\n",
    "        \n",
    "        loss = criterion(outputs, y_batch)  # Calculate loss\n",
    "        loss.backward()  # Backpropagate gradients\n",
    "        optimizer.step()  # Update model parameters\n",
    "        \n",
    "        total_loss += loss.item()  # Accumulate total loss\n",
    "        batch_losses.append(loss.item())  # Store individual batch losses\n",
    "        \n",
    "        # Convert outputs to predicted labels (highest logit for each sample)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get predicted labels (indices of max logits)\n",
    "\n",
    "        # Collect the predictions and true labels into lists\n",
    "        train_predictions.extend(predicted.cpu().numpy())  # Flatten and add predictions\n",
    "        train_labels.extend(y_batch.cpu().numpy())        # Add true labels to the list\n",
    "    \n",
    "    # Calculate the average loss for the epoch\n",
    "    epoch_avg_loss = total_loss / len(train_loader)\n",
    "    epoch_losses.append(epoch_avg_loss)  # Store average loss per epoch\n",
    "\n",
    "    # Print loss for the current epoch\n",
    "    print(f\"Epoch {epoch+1}, Loss: {epoch_avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Optionally: Print all epoch losses after training is complete\n",
    "print(\"Epoch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2da08c2-dcc3-435d-ac61-a2e8e2bf3d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.0935\n",
      "Epoch 1, Validation Loss: 0.0121, Validation Accuracy: 99.77%\n",
      "Epoch 2, Train Loss: 0.0109\n",
      "Epoch 2, Validation Loss: 0.0052, Validation Accuracy: 99.92%\n",
      "Epoch 3, Train Loss: 0.0048\n",
      "Epoch 3, Validation Loss: 0.0088, Validation Accuracy: 99.72%\n",
      "Epoch 4, Train Loss: 0.0040\n",
      "Epoch 4, Validation Loss: 0.0074, Validation Accuracy: 99.88%\n",
      "Epoch 5, Train Loss: 0.0024\n",
      "Epoch 5, Validation Loss: 0.0025, Validation Accuracy: 99.95%\n",
      "Epoch 6, Train Loss: 0.0019\n",
      "Epoch 6, Validation Loss: 0.0047, Validation Accuracy: 99.91%\n",
      "Epoch 7, Train Loss: 0.0020\n",
      "Epoch 7, Validation Loss: 0.0275, Validation Accuracy: 99.24%\n",
      "Epoch 8, Train Loss: 0.0017\n",
      "Epoch 8, Validation Loss: 0.0050, Validation Accuracy: 99.93%\n",
      "Epoch 9, Train Loss: 0.0017\n",
      "Epoch 9, Validation Loss: 0.0056, Validation Accuracy: 99.94%\n",
      "Epoch 10, Train Loss: 0.0013\n",
      "Epoch 10, Validation Loss: 0.0029, Validation Accuracy: 99.96%\n",
      "Training complete!\n",
      "Best model achieved at epoch 10 with validation accuracy: 99.96%\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
    "import wandb \n",
    "\n",
    "\n",
    "# Initialize wandb logging\n",
    "wandb.init(project=\"asl_classification\", name=\"train_validation_log\")\n",
    "\n",
    "\n",
    "# Function to train the model\n",
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_losses = []\n",
    "\n",
    "    # Metrics for accuracy per epoch\n",
    "    train_predictions = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for X_batch, y_batch in train_loader: \n",
    "        optimizer.zero_grad()\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)  # Raw outputs from the model\n",
    "        \n",
    "        loss = criterion(outputs, y_batch)  # Calculate loss\n",
    "        loss.backward()  # Backpropagate gradients\n",
    "        optimizer.step()  # Update model parameters\n",
    "        \n",
    "        total_loss += loss.item()  # Accumulate total loss\n",
    "        batch_losses.append(loss.item())  # Store individual batch losses\n",
    "        \n",
    "        # Convert outputs to predicted labels (highest logit for each sample)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get predicted labels (indices of max logits)\n",
    "\n",
    "        train_predictions.append(predicted)\n",
    "        train_labels.append(y_batch)\n",
    "\n",
    "    # Calculate the average loss for the epoch\n",
    "    epoch_avg_loss = total_loss / len(train_loader)\n",
    "    return epoch_avg_loss, train_predictions, train_labels\n",
    "\n",
    "\n",
    "# Function to validate the model\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_predictions = []\n",
    "    val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            outputs = model(X_val_batch)\n",
    "            loss = criterion(outputs, y_val_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == y_val_batch).sum().item()\n",
    "            total += y_val_batch.size(0)\n",
    "            \n",
    "            # Collect validation predictions and labels\n",
    "            val_predictions.append(predicted)\n",
    "            val_labels.append(y_val_batch)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total  # Calculate accuracy\n",
    "\n",
    "    # Optionally calculate and log precision, recall, specificity, sensitivity\n",
    "    precision = precision_score(val_labels, val_predictions, average='weighted')\n",
    "    recall = recall_score(val_labels, val_predictions, average='weighted')\n",
    "    conf_matrix = confusion_matrix(val_labels, val_predictions)\n",
    "    specificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "\n",
    "    return avg_val_loss, val_accuracy, precision, recall, specificity, sensitivity\n",
    "\n",
    "\n",
    "# Main training and validation loop\n",
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    # Save metrics for accuracy per epoch\n",
    "    epoch_losses = []  # Loss values per epoch\n",
    "    best_val_accuracy = 0.0\n",
    "    best_epoch = 0\n",
    "    best_model_predictions = []\n",
    "    best_model_labels = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train the model\n",
    "        train_avg_loss, train_predictions, train_labels = train(model, train_loader, criterion, optimizer, device)\n",
    "        epoch_losses.append(train_avg_loss)\n",
    "        \n",
    "        # Log training loss\n",
    "        wandb.log({\n",
    "            \"train_loss\": train_avg_loss\n",
    "        })\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_avg_loss:.4f}\")\n",
    "\n",
    "        # Validate the model\n",
    "        val_avg_loss, val_accuracy, precision, recall, specificity, sensitivity = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        # Log validation metrics\n",
    "        wandb.log({\n",
    "            \"val_loss\": val_avg_loss,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"specificity\": specificity,\n",
    "            \"sensitivity\": sensitivity\n",
    "        })\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Validation Loss: {val_avg_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "        \n",
    "        # Save the model with the best validation accuracy\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_epoch = epoch + 1\n",
    "            best_model_predictions = train_predictions.copy()\n",
    "            best_model_labels = train_labels.copy()\n",
    "            \n",
    "            # Save the model state (best model weights)\n",
    "            torch.save(model.state_dict(), f'best_model_{current_time}.pth')\n",
    "\n",
    "    # Print training results after all epochs\n",
    "    print(\"Training complete!\")\n",
    "    print(f\"Best model achieved at epoch {best_epoch} with validation accuracy: {best_val_accuracy:.2f}%\")\n",
    "    \n",
    "    return model, best_model_predictions, best_model_labels, epoch_losses\n",
    "\n",
    "\n",
    "# Example:\n",
    "# Must be defined already: model, criterion, optimizer, and data loaders (train_loader, val_loader)\n",
    "model = AslCNN(num_classes=27).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Call the train_and_validate function to train and validate the model\n",
    "trained_model, best_model_predictions, best_model_labels, epoch_losses = train_and_validate(\n",
    "    model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "80d85783-5adc-4ce6-929c-9a45ff6290e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 99.96%\n"
     ]
    }
   ],
   "source": [
    "X_test_tensor = X_test_tensor.view(-1, 1, 21, 3) # already done above\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41f5d0b-1884-464d-8289-b96133fa9c22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
