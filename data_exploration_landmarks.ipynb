{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57fba083-ae77-4963-8cab-3432ce9cbd7a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92ab588d-ef5f-43d5-abf0-85eaa2d5f026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipympl in /opt/conda/lib/python3.12/site-packages (0.9.6)\n",
      "Requirement already satisfied: ipython<9 in /opt/conda/lib/python3.12/site-packages (from ipympl) (8.32.0)\n",
      "Requirement already satisfied: ipywidgets<9,>=7.6.0 in /opt/conda/lib/python3.12/site-packages (from ipympl) (8.1.5)\n",
      "Requirement already satisfied: matplotlib<4,>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from ipympl) (3.10.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from ipympl) (2.1.3)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.12/site-packages (from ipympl) (11.1.0)\n",
      "Requirement already satisfied: traitlets<6 in /opt/conda/lib/python3.12/site-packages (from ipympl) (5.14.3)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.12/site-packages (from ipython<9->ipympl) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.12/site-packages (from ipython<9->ipympl) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.12/site-packages (from ipython<9->ipympl) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.12/site-packages (from ipython<9->ipympl) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.12/site-packages (from ipython<9->ipympl) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.12/site-packages (from ipython<9->ipympl) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /opt/conda/lib/python3.12/site-packages (from ipython<9->ipympl) (0.6.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.12/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (0.2.2)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /opt/conda/lib/python3.12/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.12 in /opt/conda/lib/python3.12/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (3.0.13)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (2.9.0.post0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/conda/lib/python3.12/site-packages (from jedi>=0.16->ipython<9->ipympl) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.12/site-packages (from pexpect>4.3->ipython<9->ipympl) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython<9->ipympl) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib<4,>=3.5.0->ipympl) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.12/site-packages (from stack_data->ipython<9->ipympl) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.12/site-packages (from stack_data->ipython<9->ipympl) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /opt/conda/lib/python3.12/site-packages (from stack_data->ipython<9->ipympl) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.12/site-packages (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wandb in /opt/conda/lib/python3.12/site-packages (0.19.9)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.12/site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.12/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.12/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /opt/conda/lib/python3.12/site-packages (from wandb) (5.28.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.12/site-packages (from wandb) (6.1.1)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.12/site-packages (from wandb) (2.10.6)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.12/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from wandb) (2.26.1)\n",
      "Requirement already satisfied: setproctitle in /opt/conda/lib/python3.12/site-packages (from wandb) (1.3.5)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from wandb) (75.8.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /opt/conda/lib/python3.12/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.12/site-packages (from pydantic<3->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipympl\n",
    "%pip install python-dotenv\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2698210-718a-4ea6-b9fe-90bdadcbed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72472726-3af3-441f-97b5-e82af760355d",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038ba933-3c3b-4379-af05-a63266387165",
   "metadata": {},
   "source": [
    "## Landmark data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a32d740-92fc-49d2-9bad-a6e454b504b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/exchange/dspro2/silent-speech/ASL_Landmarks_Dataset/Train/X_train.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/exchange/dspro2/silent-speech/ASL_Landmarks_Dataset/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#'/exchange/dspro2/silent-speech/full-dataset/'\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Full dataset\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Split data for comparison with other models\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTrain/X_train.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m X_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(dataset_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation/X_val.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(dataset_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest/X_test.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:459\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    457\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    460\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/exchange/dspro2/silent-speech/ASL_Landmarks_Dataset/Train/X_train.npy'"
     ]
    }
   ],
   "source": [
    "dataset_path = '/exchange/dspro2/silent-speech/ASL_Landmarks_Dataset/'\n",
    "#'/exchange/dspro2/silent-speech/full-dataset/'\n",
    "\n",
    "# Full dataset\n",
    "#X = np.load(dataset_path + \"X_landmarks.npy\")\n",
    "#y = np.load(dataset_path + \"y_labels.npy\") #['A' 'B' 'C' 'D' 'E' 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R''S' 'Space' 'T' 'U' 'V' 'W' 'X' 'Y' 'Z' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h'\n",
    " #'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 'u' 'v' 'w' 'x' 'y' 'z']\n",
    "\n",
    "# Split data for comparison with other models\n",
    "X_train = np.load(dataset_path + \"Train/X_train.npy\")\n",
    "X_val = np.load(dataset_path + \"Validation/X_val.npy\")\n",
    "X_test = np.load(dataset_path + \"Test/X_test.npy\")\n",
    "y_train = np.load(dataset_path + \"Train/y_train.npy\")\n",
    "y_val = np.load(dataset_path + \"Validation/y_val.npy\")\n",
    "y_test = np.load(dataset_path + \"Test/y_test.npy\")\n",
    "\n",
    "y = np.array([label.upper() for label in y])\n",
    "print(f'X shape: {X.shape}')\n",
    "print(f'X_train shape {X_train.shape}')\n",
    "print(f'X_validation shape {X_val.shape}')\n",
    "print(f'X_test shape {X_test.shape}')\n",
    "assert ((X_train.shape[0] + X_val.shape[0] + X_test.shape[0]) == X.shape[0]), \"Mismatch between X length and X splits length\"\n",
    "\n",
    "print(f'y[0] {y[0]}, y shape {y.shape}')\n",
    "print(f'y_train shape {y_train.shape}')\n",
    "print(f'y_val {y_val.shape}')\n",
    "print(f'y_test {y_test.shape}')\n",
    "assert ((y_train.shape[0]+ y_val.shape[0] + y_test.shape[0]) == y.shape[0]), \"Mismatch between X length and X splits length\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dd5077-4080-4a2c-86cd-986e3e57dbad",
   "metadata": {},
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e6e795-c8c6-4c4c-a115-c049130ff7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "class_counts = pd.Series(y_train).value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.barplot(x=class_counts.index, y=class_counts.values, color='blue')#, palette=\"viridis\")\n",
    "plt.xlabel(\"Letters\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.title(\"Classes' distribution on train set\")\n",
    "plt.xticks(rotation=45)  # Rotate labels if crowded\n",
    "plt.show()\n",
    "\n",
    "assert (class_counts.sum() == y_train.shape), \"class_counts does not correspond to y_train length\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336d25cb-7235-430a-a81d-63bab84d2bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Normalize the landmarks (Standardization: mean=0, std=1)\n",
    "# Ignores the 3D structure. To let go.\n",
    "scaler = StandardScaler()\n",
    "X_train_norm1 = scaler.fit_transform(X_train)  # No reshape needed\n",
    "X_val_norm1 = scaler.transform(X_val)\n",
    "X_test_norm1 = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Fit scaler on all x/y/z coordinates, by axis.\n",
    "# Good for outliers\n",
    "scaler = StandardScaler()\n",
    "# Reshape to (n_samples * 21 poses, 3 coordinates)\n",
    "X_train_reshaped = X_train.reshape(-1, 3)\n",
    "X_train_scaled = scaler.fit_transform(X_train_reshaped)\n",
    "# Reshape back to original (n_samples, 63) format\n",
    "X_train_norm2 = X_train_scaled.reshape(X_train.shape)\n",
    "\n",
    "X_val_reshaped = X_val.reshape(-1, 3)\n",
    "#No fit transform to avoid bias the validation\n",
    "# TODO CHECK\n",
    "X_val_scaled = scaler.fit_transform(X_val_reshaped)\n",
    "# Reshape back to original (n_samples, 63) format\n",
    "X_val_norm2 = X_val_scaled.reshape(X_val.shape)\n",
    "\n",
    "\n",
    "# To normalize from different distances, avoids size bias\n",
    "# A utiliser\n",
    "# Reshape to (n_samples * 21 poses, 3 coordinates)\n",
    "X_train_reshaped = X_train.reshape(-1, 3)\n",
    "# Center to origin\n",
    "X_train_centered = X_train_reshaped - X_train_reshaped.mean(axis=0)\n",
    "# Scale to unit sphere\n",
    "max_train_distance = np.max(np.linalg.norm(X_train_centered, axis=1))\n",
    "X_train_normalized = X_train_centered / max_train_distance\n",
    "# Reshape back\n",
    "X_train_norm3 = X_train_normalized.reshape(X_train.shape)\n",
    "\n",
    "\n",
    "X_val_reshaped = X_val.reshape(-1, 3)\n",
    "# Center to origin\n",
    "X_val_centered = X_val_reshaped - X_val_reshaped.mean(axis=0)\n",
    "# Scale to unit sphere\n",
    "X_val_normalized = X_val_centered / max_train_distance\n",
    "# Reshape back\n",
    "X_val_norm3 = X_val_normalized.reshape(X_val.shape)\n",
    "\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 6))  # 2 rows, 2 columns, size 18x6 for the entire figure\n",
    "\n",
    "# Plotting for each subplot (example with boxplots)\n",
    "sns.boxplot(data=X_train, ax=axes[0, 0])  # Top-left subplot\n",
    "axes[0, 0].set_title('Standardization z-score')\n",
    "\n",
    "sns.boxplot(data=X_train_norm1, ax=axes[0, 1])  # Top-right subplot\n",
    "axes[0, 1].set_title('Normalization by axis')\n",
    "\n",
    "sns.boxplot(data=X_train_norm2, ax=axes[1, 0])  # Bottom-left subplot\n",
    "axes[1, 0].set_title('Against outliers')\n",
    "\n",
    "sns.boxplot(data=X_train_norm3, ax=axes[1, 1])  # Bottom-right subplot\n",
    "axes[1, 1].set_title('For 3D poses')\n",
    "\n",
    "# Adjust the layout to avoid overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754a1739-b78a-4b1f-b0f1-29450c3b54eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.cm as cm # Color map\n",
    "# 63 hand poses, 21 points/pose, 3 coordinates per point\n",
    "X_train_samples = X_train[:10]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "n = 111\n",
    "\n",
    "# Color Map\n",
    "cmap = plt.get_cmap()\n",
    "\n",
    "for index, pose in enumerate(X_train_samples):\n",
    "    # Reshape in 21x3\n",
    "    points = pose.reshape(-1,3)\n",
    "    x, y, z = points[:, 0], points[:,1], points[:,2]\n",
    "    color = cmap(index / len(X_train_samples)) # NOrmalize index to [0,1]\n",
    "    ax.scatter(x,y,z, marker='o', color=color)\n",
    "    #for i in range(len(x)-1):\n",
    "    #    ax.plot([x[i], x[i+1]], \n",
    "    #            [y[i], y[i+1]],\n",
    "    #            [z[i], z[i+1]], color=color)\n",
    "    \n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "ax.set_title('Hand poses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014d407a-b7a0-439b-b7e7-cce3089c02fe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.cm as cm # Color map\n",
    "# 63 hand poses, 21 points/pose, 3 coordinates per point\n",
    "X_train_samples = X_train[2:4]\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "n = 111\n",
    "\n",
    "# Color Map\n",
    "cmap = plt.get_cmap()\n",
    "\n",
    "for index, pose in enumerate(X_train_samples):\n",
    "    # Reshape in 21x3\n",
    "    points = pose.reshape(-1,3)\n",
    "    x, y, z = points[:, 0], points[:,1], points[:,2]\n",
    "    color = cmap(index / len(X_train_samples)) # Normalize index to [0,1]\n",
    "    ax.scatter(x,y,z, marker='o', color=color)\n",
    "    for i in range(len(x)-1):\n",
    "        ax.plot([x[i], x[i+1]], \n",
    "                [y[i], y[i+1]],\n",
    "                [z[i], z[i+1]], color=color, linestyle='-')\n",
    "    \n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "ax.set_title('Hand poses')\n",
    "\n",
    "ax.view_init(elev=20, azim=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dbe4a4-d4ad-4eb6-97ee-f1be419c0116",
   "metadata": {},
   "source": [
    "## Data transformation into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dd6c3a-3611-4222-9dc6-602e9f6ef39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "# Transform into tensors\n",
    "#X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "#y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "#X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "#y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "#X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "#y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "def np_into_tensor_gpu(np_array):\n",
    "    device = torch.device(\"cuda\")\n",
    "    np_array = torch.tensor(np_array, dtype=torch.float32).to(device)\n",
    "    return np_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6655c70d-5559-4c11-bc77-109e8efa3a62",
   "metadata": {},
   "source": [
    "## Images data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524592fb-a2f5-4e79-b1e1-61ccc330566d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "592944eb-66a7-4906-8011-60d9d9867df9",
   "metadata": {},
   "source": [
    "## CNN Landmarks Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2776225b-7d87-4be9-88c5-2491d7e57c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Landmarks\n",
    "class AslCNN_landmarks(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(AslCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=3, out_channels=16, kernel_size=3, padding=1) # padding=1\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, padding=1) # padding=1\n",
    "        self.fc1 = nn.Linear(32 * 21, 128) #(32 * 17, 128)  # Adjusted fully connected layer\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(1)\n",
    "        x = x.permute(0, 2, 1)  # Reshape from (batch, 21, 3) â†’ (batch, 3, 21)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b9306f-3779-401e-a7c9-439b65be8692",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## CNN images model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a1e60a-c4af-4a4f-8608-cbf32c40fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AslCNN_img(nn.Module):\n",
    "    def __init__(self, num_classes, kernel_size=3, padding=1, channels=[16,32,64]):\n",
    "        super(AslCNN_img, self).__init__()\n",
    "\n",
    "    \n",
    "        self.conv1 = nn.Conv2d(3, channels[0], kernel_size, padding)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "    \n",
    "        self.conv2 = nn.Conv2d(channels[0], channels[1], kernel_size, padding)\n",
    "        self.conv3 = nn.Conv2d(channels[1], channels[2], kernel_size, padding)\n",
    "\n",
    "        # For 400x400 images size and 3 pooling layers\n",
    "        final_spatial_dim = 400 // (2**3) \n",
    "        flattened_size = channels[2] * final_spatial_dim * final_spatial_dim\n",
    "    \n",
    "        self.fc1 = nn.Linear(flattened_size, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes) # Number of classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3e93b2-f05d-4e86-a54d-b30dcb9b439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac10a28-cca7-41f7-8564-b5ce7c5e1e67",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8016ac3-c19e-44af-9f6d-a6de74a21a11",
   "metadata": {},
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# Create DataLoaders\n",
    "X_train_tensor = X_train_tensor.view(-1, 1, 21, 3)  # Adjust this based on expected shape (4D and not 2D)\n",
    "X_val_tensor = X_val_tensor.view(-1, 1, 21, 3)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f'X_train_tensor shape: {X_train_tensor.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdfc750-da1f-4633-9d15-30f4edced6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Create DataLoaders\n",
    "X_train3 = np_into_tensor_gpu(X_train_norm3)\n",
    "X_train3 = X_train3.view(-1, 1, 21, 3)  # Adjust this based on expected shape (4D and not 2D)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.long).to(device)\n",
    "\n",
    "X_val3 = np_into_tensor_gpu(X_val_norm3)\n",
    "X_val3 = X_val3.view(-1, 1, 21, 3)\n",
    "\n",
    "\n",
    "train_dataset3 = TensorDataset(X_train3, y_train)\n",
    "val_dataset3 = TensorDataset(X_val3, y_val)\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset3, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset3, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f'X_train_tensor shape: {X_train3.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46dbc22-1331-4c30-847f-5ccb9428fe2b",
   "metadata": {},
   "source": [
    "## Training, validation, test loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16db09ae-0e9d-4826-b09f-46d47c634e1e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# Define Model, Loss, and Optimizer\n",
    "num_classes = len(torch.unique(y_train_tensor)) # 27\n",
    "model = AslCNN(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Save metrics for accuracy\n",
    "train_predictions = []\n",
    "train_labels = []\n",
    "train_loss = []\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader: \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch) # goes in the AslCNN\n",
    "        \n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        train_loss.append(total_loss/len(train_loader))\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1e2a99-8763-4259-9a4b-08727eb8c920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score\n",
    "import wandb \n",
    "\n",
    "\n",
    "\n",
    "# Function to train the model\n",
    "def train(model, train_loader, criterion, optimizer):\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_losses = []\n",
    "\n",
    "    # Metrics for accuracy per epoch\n",
    "    train_predictions = []\n",
    "    train_labels = []\n",
    "\n",
    "    \n",
    "    for X_batch, y_batch in train_loader: \n",
    "\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # Reset gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Raw outputs from the model\n",
    "        # Forward pass automatically called by Pytorch\n",
    "        outputs = model(X_batch)  \n",
    "        # Loss from outputs        \n",
    "        loss = criterion(outputs, y_batch) \n",
    "\n",
    "        #Backward pass\n",
    "        loss.backward()  # Backpropagate gradients\n",
    "        optimizer.step()  # Update model parameters\n",
    "\n",
    "        #End of training loop\n",
    "        \n",
    "        # Log metrics\n",
    "        total_loss += loss.item()  # Accumulate total loss\n",
    "        #batch_losses.append(loss.item())  # Store individual batch losses\n",
    "        \n",
    "        # Convert outputs to predicted labels (highest logit for each sample)\n",
    "    # TODO put it before the backprop\n",
    "        _, predicted = torch.max(outputs, 1)  # Get predicted labels (indices of max logits)\n",
    "\n",
    "        train_predictions.append(predicted)\n",
    "        train_labels.append(y_batch)\n",
    "\n",
    "    # Calculate the average loss for the epoch\n",
    "    epoch_avg_loss = total_loss / len(train_loader)\n",
    "    return epoch_avg_loss, train_predictions, train_labels\n",
    "\n",
    "\n",
    "# Function to validate the model\n",
    "def validate(model, val_loader, criterion):\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_predictions = []\n",
    "    val_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            outputs = model(X_val_batch)\n",
    "            loss = criterion(outputs, y_val_batch)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == y_val_batch).sum().item()\n",
    "            total += y_val_batch.size(0)\n",
    "            \n",
    "            # Move tensors to CPU before appending\n",
    "            val_predictions.append(predicted.cpu())  \n",
    "            val_labels.append(y_val_batch.cpu())  \n",
    "\n",
    "    # Convert tensors to numpy arrays after moving to CPU\n",
    "    val_predictions = torch.cat(val_predictions).cpu().numpy()\n",
    "    val_labels = torch.cat(val_labels).cpu().numpy()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total  # Calculate accuracy\n",
    "\n",
    "    # Calculate precision, recall\n",
    "    precision = precision_score(val_labels, val_predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(val_labels, val_predictions, average='weighted', zero_division=0)\n",
    "    conf_matrix = confusion_matrix(val_labels, val_predictions)\n",
    "\n",
    "    # Calculate specificity (for binary classification)\n",
    "    if conf_matrix.shape == (2, 2):\n",
    "        specificity = conf_matrix[0, 0] / (conf_matrix[0, 0] + conf_matrix[0, 1])\n",
    "    else:\n",
    "        specificity = 0  # Not applicable for multi-class\n",
    "    \n",
    "    sensitivity = recall  # Equivalent to recall for binary classification\n",
    "\n",
    "    return avg_val_loss, val_accuracy, precision, recall, specificity, sensitivity, sk_learn_acc\n",
    "\n",
    "\n",
    "\n",
    "# Main training and validation loop\n",
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    device = torch.device(\"cuda\")\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    # Save metrics for accuracy per epoch\n",
    "    epoch_losses = []  # Loss values per epoch\n",
    "    best_val_accuracy = 0.0\n",
    "    best_epoch = 0\n",
    "    best_model_predictions = []\n",
    "    best_model_labels = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train the model\n",
    "        train_avg_loss, train_predictions, train_labels = train(model, train_loader, criterion, optimizer)\n",
    "        epoch_losses.append(train_avg_loss)\n",
    "        \n",
    "        # Log training loss\n",
    "        wandb.log({\n",
    "            \"train_loss\": train_avg_loss\n",
    "        })\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_avg_loss:.4f}\")\n",
    "\n",
    "        # Validate the model\n",
    "        val_avg_loss, val_accuracy, precision, recall, specificity, sensitivity = validate(model, val_loader, criterion)\n",
    "\n",
    "        # Log validation metrics\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch +1,\n",
    "            \"val_loss\": val_avg_loss,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"specificity\": specificity,\n",
    "            \"sensitivity\": sensitivity\n",
    "        })\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Validation Loss: {val_avg_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "        \n",
    "        # Save the model with the best validation accuracy\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_epoch = epoch + 1\n",
    "            best_model_predictions = train_predictions.copy()\n",
    "            best_model_labels = train_labels.copy()\n",
    "            \n",
    "            # Save the model state (best model weights)\n",
    "            torch.save(model.state_dict(), f'best_model_{current_time}.pth')\n",
    "\n",
    "    # Print training results after all epochs\n",
    "    print(\"Training complete!\")\n",
    "    print(f\"Best model achieved at epoch {best_epoch} with validation accuracy: {best_val_accuracy:.2f}%\")\n",
    "    \n",
    "    return model, best_model_predictions, best_model_labels, epoch_losses, sk_learn_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e73f1c7-6b1c-4524-9779-5f595cafd5ad",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# CNN IMAGES\n",
    "\n",
    "num_classes = 28 \n",
    "device = torch.device(\"cuda\")\n",
    "model = AslCNN_img(num_classes=num_classes, kernel_size=3, padding=1, channels=[16,32,64]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001) #weight_decay = regularization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Call the train_and_validate function to train and validate the model\n",
    "trained_model, best_model_predictions, best_model_labels, epoch_losses, sk_learn_acc = train_and_validate(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa719ec-6415-4282-9459-f3b21466f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "# check the .env file \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get and print the WANDB_API_KEY\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY_DSPRO02\")\n",
    "#print(f\"WANDB_API_KEY: [{wandb_api_key[:4]}...]\")\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f784993f-8147-4424-abd5-50d9009d9f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb logging\n",
    "run_nr = 1\n",
    "run = wandb.init(project=\"ASL_CNN\", \n",
    "                 name=f\"CNN_test_standardization3_{run_nr}\",\n",
    "                 config={\"Conv1d\": \"in=3, out=16, kernel=3, pad=1\",\n",
    "                         \"Conv1d_2\": \"in=16, out=32, kernel=3, pad=1\",\n",
    "                         \"fc1\": (32*21, 128),\n",
    "                         \"fc2\": (128, 27),\n",
    "                         \"activations\": \"ReLU\",\n",
    "                         \"epoch\": 10,\n",
    "                         \"batch_size\": 1024,\n",
    "                         \"optimizer\": \"adam\",\n",
    "                         \"learning_rate\": 1e-3,\n",
    "                         \"criterion\": \"cross_entropy\",\n",
    "                         \"weight_decay\": 1e-4,  })\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "num_classes = len(torch.unique(y_train)) # 27\n",
    "device = torch.device(\"cuda\")\n",
    "model = AslCNN_landmarks(num_classes=num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay) #weight_decay = regularization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Call the train_and_validate function to train and validate the model\n",
    "trained_model, best_model_predictions, best_model_labels, epoch_losses = train_and_validate(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs=config.epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0182545b-e3eb-4fa0-843e-4ff79656b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish() # Except if testing after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d85783-5adc-4ce6-929c-9a45ff6290e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = X_test_tensor.view(-1, 1, 21, 3) # already done above\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9299af-4a46-4179-917c-dbe8710cb41b",
   "metadata": {},
   "source": [
    "## Search for data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50371098-c149-41d6-93ed-74294bc286f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if data leakage\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Full dataset tensors\n",
    "indices = np.arange(len(X))  # Create index list\n",
    "print(indices)\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert indices to sets for comparison\n",
    "train_indices = set(train_idx)\n",
    "val_indices = set(val_idx)\n",
    "\n",
    "print(\"Train and Val Overlap:\", train_indices.intersection(val_indices))  # Should be empty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce948ed2-4ccb-4ae6-8baa-3c776a63b112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt with shuffled y to search bias in the model - can it learn the noise?\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "y_shuffled = y_train_tensor[torch.randperm(y_train_tensor.size(0))]  # Shuffle labels\n",
    "\n",
    "if not isinstance(y_shuffled, torch.Tensor):\n",
    "    y_shuffled = torch.tensor(y_shuffled, dtype=torch.long).to(device)\n",
    "    \n",
    "# Create a new dataset with shuffled labels\n",
    "shuffled_dataset = TensorDataset(X_train_tensor, y_shuffled)\n",
    "\n",
    "# Load into DataLoader\n",
    "shuffled_loader = DataLoader(shuffled_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize wandb logging\n",
    "run_nr = 1\n",
    "run = wandb.init(project=\"ASL_CNN\", \n",
    "                 name=f\"CNN_random_leakage_search_{run_nr}\",\n",
    "                 config={\"Conv1d\": \"in=3, out=16, kernel=3, pad=1\",\n",
    "                         \"Conv1d_2\": \"in=16, out=32, kernel=3, pad=1\",\n",
    "                         \"fc1\": (32*21, 128),\n",
    "                         \"fc2\": (128, 27),\n",
    "                         \"activations\": \"ReLU\",\n",
    "                         \"epoch\": 10,\n",
    "                         \"batch_size\": 1024,\n",
    "                         \"optimizer\": \"adam\",\n",
    "                         \"learning_rate\": 1e-3,\n",
    "                         \"criterion\": \"cross_entropy\",\n",
    "                         \"weight_decay\": 1e-4,  })\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "num_classes = len(torch.unique(y_train_tensor)) \n",
    "device = torch.device(\"cuda\")\n",
    "model = AslCNN(num_classes=num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay) #weight_decay = regularization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Call the train_and_validate function to train and validate the model\n",
    "trained_model, best_model_predictions, best_model_labels, epoch_losses = train_and_validate(\n",
    "    model, shuffled_loader, val_loader, criterion, optimizer, num_epochs=config.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92380482-6143-4edc-855d-3833251ce7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
